{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\sahil\\\\OneDrive\\\\Desktop\\\\AutoSummaryAI\\\\AutoSummaryAI\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\sahil\\\\OneDrive\\\\Desktop\\\\AutoSummaryAI\\\\AutoSummaryAI'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: float\n",
    "    gradient_accumulation_steps: int\n",
    "    learning_rate: float\n",
    "    load_best_model_at_end: bool\n",
    "    metric_for_best_model: str\n",
    "    greater_is_better: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoSummaryAI.constants import *\n",
    "from AutoSummaryAI.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            num_train_epochs=params.num_train_epochs,\n",
    "            warmup_steps=params.warmup_steps,\n",
    "            per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=params.per_device_eval_batch_size,\n",
    "            weight_decay=params.weight_decay,\n",
    "            logging_steps=params.logging_steps,\n",
    "            evaluation_strategy=params.evaluation_strategy,\n",
    "            eval_steps=params.eval_steps,\n",
    "            save_steps=params.save_steps,\n",
    "            gradient_accumulation_steps=params.gradient_accumulation_steps,\n",
    "            learning_rate=params.learning_rate,\n",
    "            load_best_model_at_end=params.load_best_model_at_end,\n",
    "            metric_for_best_model=params.metric_for_best_model,\n",
    "            greater_is_better=params.greater_is_better\n",
    "    )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from bitsandbytes) (2.6.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from bitsandbytes) (2.2.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from peft) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from peft) (2.6.0+cu126)\n",
      "Requirement already satisfied: transformers in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch>=1.13.0->peft) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sahil\\anaconda3\\envs\\autosumai\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahil\\anaconda3\\envs\\autosumai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-30 09:01:57,954: INFO: config: PyTorch version 2.6.0+cu126 available.]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model with extreme GPU memory optimization techniques for 4GB GPUs\n",
    "        using Parameter-Efficient Fine-Tuning (PEFT) with LoRA and ROUGE metrics\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        import gc\n",
    "        import numpy as np\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "        \n",
    "        # First, install required packages if not already installed\n",
    "        try:\n",
    "            import peft\n",
    "            import rouge_score\n",
    "        except ImportError:\n",
    "            import subprocess\n",
    "            import sys\n",
    "            print(\"Installing required packages...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"peft\", \"rouge-score\"])\n",
    "            import peft\n",
    "            import rouge_score\n",
    "            \n",
    "        # Force aggressive garbage collection and memory cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Report initial memory state\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Initial GPU memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB allocated\")\n",
    "        \n",
    "        # Set device to CUDA if available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Load tokenizer first (uses less memory)\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        \n",
    "        # Setup 4-bit quantization configuration (more aggressive than 8-bit)\n",
    "        print(\"Configuring 4-bit quantization...\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,              # Enable 4-bit quantization (more memory efficient)\n",
    "            bnb_4bit_use_double_quant=True, # Use nested quantization for 4-bit weights\n",
    "            bnb_4bit_quant_type=\"nf4\",      # Use normalized float 4 format\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16  # Use bfloat16 for computation with 4-bit\n",
    "        )\n",
    "        \n",
    "        # Load model with extreme memory optimization\n",
    "        print(\"Loading model with 4-bit quantization...\")\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            self.config.model_ckpt,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        # Immediately prepare model for k-bit training\n",
    "        print(\"Preparing model for training with PEFT/LoRA...\")\n",
    "        model_pegasus = prepare_model_for_kbit_training(model_pegasus)\n",
    "        \n",
    "        # Define LoRA configuration with memory-efficient parameters\n",
    "        print(\"Configuring LoRA with reduced parameters...\")\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "            r=4,                        # Reduced rank (was 8)\n",
    "            lora_alpha=16,              # Reduced alpha (was 32)\n",
    "            lora_dropout=0.05,          # Reduced dropout (was 0.1)\n",
    "            # Apply to fewer modules for memory efficiency\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        \n",
    "        # Apply LoRA adapter\n",
    "        print(\"Applying LoRA adapter to the model...\")\n",
    "        model_pegasus = get_peft_model(model_pegasus, lora_config)\n",
    "        \n",
    "        # Enable gradient checkpointing (trades computation for memory)\n",
    "        print(\"Enabling gradient checkpointing...\")\n",
    "        model_pegasus.gradient_checkpointing_enable()\n",
    "        \n",
    "        # Print trainable parameters information\n",
    "        self.print_trainable_parameters(model_pegasus)\n",
    "        \n",
    "        # Set extreme small batch sizes for 4GB GPU\n",
    "        train_batch_size = 1  # Can't go lower than 1\n",
    "        eval_batch_size = 1\n",
    "        \n",
    "        # Use higher gradient accumulation\n",
    "        gradient_accumulation_steps = 32  # Increased from 16 to 32\n",
    "        \n",
    "        print(f\"GPU memory after loading model: {torch.cuda.memory_allocated()/1024**2:.2f} MB allocated\")\n",
    "        \n",
    "        # Data collator that will handle dynamic padding\n",
    "        print(\"Creating data collator...\")\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus, padding=\"longest\")\n",
    "        \n",
    "        # Load dataset\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "        \n",
    "        # Use a smaller subset of training and validation data for testing\n",
    "        # This helps with memory usage and speeds up initial validation\n",
    "        train_subset_size = min(1000, len(dataset_samsum_pt[\"train\"]))\n",
    "        print(f\"Using {train_subset_size} examples for training (out of {len(dataset_samsum_pt['train'])})\")\n",
    "        train_dataset = dataset_samsum_pt[\"train\"].select(range(train_subset_size))\n",
    "        \n",
    "        # Use very small validation set\n",
    "        if \"validation\" in dataset_samsum_pt:\n",
    "            val_subset_size = min(50, len(dataset_samsum_pt[\"validation\"]))\n",
    "            print(f\"Using {val_subset_size} examples for validation (out of {len(dataset_samsum_pt['validation'])})\")\n",
    "            val_dataset = dataset_samsum_pt[\"validation\"].select(range(val_subset_size))\n",
    "        else:\n",
    "            val_dataset = None\n",
    "        \n",
    "        # Define compute_metrics function for ROUGE evaluation\n",
    "        def compute_metrics(pred):\n",
    "            \"\"\"\n",
    "            Compute ROUGE metrics for summarization model evaluation\n",
    "            with minimal memory usage.\n",
    "            \"\"\"\n",
    "            from rouge_score import rouge_scorer\n",
    "            \n",
    "            # Process predictions in small batches to save memory\n",
    "            labels = []\n",
    "            predictions = []\n",
    "            \n",
    "            # Process in batches of 8 to avoid memory spikes\n",
    "            batch_size = 8\n",
    "            \n",
    "            for i in range(0, len(pred.predictions), batch_size):\n",
    "                # Get batch\n",
    "                pred_batch = pred.predictions[i:i + batch_size]\n",
    "                label_batch = pred.label_ids[i:i + batch_size]\n",
    "                \n",
    "                # Decode predictions\n",
    "                pred_texts = tokenizer.batch_decode(\n",
    "                    pred_batch, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "                predictions.extend(pred_texts)\n",
    "                \n",
    "                # Process labels - replace -100 padding\n",
    "                proc_labels = []\n",
    "                for label in label_batch:\n",
    "                    # Replace -100 with pad token ID\n",
    "                    label = np.where(label != -100, label, tokenizer.pad_token_id)\n",
    "                    proc_labels.append(label)\n",
    "                \n",
    "                # Decode labels\n",
    "                label_texts = tokenizer.batch_decode(\n",
    "                    proc_labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "                labels.extend(label_texts)\n",
    "                \n",
    "                # Force garbage collection after each batch\n",
    "                if i % (batch_size * 4) == 0:\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # Calculate ROUGE scores\n",
    "            scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "            \n",
    "            # Calculate scores for each prediction and reference pair\n",
    "            rouge1_scores = []\n",
    "            rouge2_scores = []\n",
    "            rougeL_scores = []\n",
    "            \n",
    "            # Process in batches to avoid memory spikes\n",
    "            for i in range(len(predictions)):\n",
    "                # Handle empty predictions or references gracefully\n",
    "                if not predictions[i].strip() or not labels[i].strip():\n",
    "                    rouge1_scores.append(0)\n",
    "                    rouge2_scores.append(0)\n",
    "                    rougeL_scores.append(0)\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate scores\n",
    "                try:\n",
    "                    scores = scorer.score(labels[i], predictions[i])\n",
    "                    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "                    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "                    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating ROUGE: {e}\")\n",
    "                    rouge1_scores.append(0)\n",
    "                    rouge2_scores.append(0)\n",
    "                    rougeL_scores.append(0)\n",
    "                \n",
    "                # Force garbage collection periodically\n",
    "                if i % 20 == 0:\n",
    "                    gc.collect()\n",
    "            \n",
    "            # Return the average scores\n",
    "            results = {\n",
    "                'rouge1': float(np.mean(rouge1_scores)),\n",
    "                'rouge2': float(np.mean(rouge2_scores)),\n",
    "                'rougeL': float(np.mean(rougeL_scores))\n",
    "            }\n",
    "            print(f\"ROUGE Scores: {results}\")\n",
    "            return results\n",
    "            \n",
    "        # Set up training arguments with extreme memory optimization\n",
    "        print(f\"Setting up trainer with batch size: {train_batch_size}, grad_accum: {gradient_accumulation_steps}\")\n",
    "        \n",
    "        # Set shorter training length for testing\n",
    "        num_epochs = 1  # Start with 1 epoch to verify setup\n",
    "        \n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            logging_steps=25,  # More frequent logging\n",
    "            eval_strategy=\"steps\" if val_dataset else \"no\",\n",
    "            eval_steps=100,    # More frequent evaluation\n",
    "            save_steps=500,    # Save less frequently to reduce disk I/O\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            learning_rate=float(self.config.learning_rate),\n",
    "            load_best_model_at_end=True if val_dataset else False,\n",
    "            metric_for_best_model=\"rouge1\",  # Use ROUGE1 for model selection\n",
    "            greater_is_better=True,          # Higher ROUGE is better\n",
    "            fp16=True,                      # Use mixed precision\n",
    "            fp16_full_eval=True,            # Use mixed precision for eval too\n",
    "            gradient_checkpointing=True,    # Enable gradient checkpointing\n",
    "            optim=\"adamw_torch\",            # Use AdamW optimizer\n",
    "            max_grad_norm=0.3,              # Lower gradient clipping threshold\n",
    "            ddp_find_unused_parameters=False,\n",
    "            dataloader_pin_memory=False,    # Save CPU memory\n",
    "            report_to=\"none\",               # Disable reporting\n",
    "            # These settings help with memory issues during training\n",
    "            dataloader_num_workers=0,       # Don't use multiprocessing\n",
    "            group_by_length=True,           # Group similar length sequences\n",
    "            lr_scheduler_type=\"cosine\",     # Cosine learning rate schedule\n",
    "            # The following are to avoid \"deadlocks\"\n",
    "            use_cpu=False,                  # Don't use CPU for training\n",
    "            seed=42,                        # Fixed seed for reproducibility\n",
    "            # Debug settings - uncomment to help diagnose issues\n",
    "            # debug=\"underflow_overflow\",\n",
    "        )\n",
    "        \n",
    "        # Create trainer with compute_metrics\n",
    "        print(\"Creating trainer with ROUGE metrics...\")\n",
    "        trainer = Trainer(\n",
    "            model=model_pegasus,\n",
    "            args=trainer_args,\n",
    "            data_collator=seq2seq_data_collator,\n",
    "            train_dataset=train_dataset,  # Using the subset\n",
    "            eval_dataset=val_dataset,     # Using the subset\n",
    "            compute_metrics=compute_metrics  # Add the ROUGE metrics calculation\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting training...\")\n",
    "        try:\n",
    "            trainer.train()\n",
    "            \n",
    "            # Save model and tokenizer\n",
    "            print(\"Saving model and tokenizer...\")\n",
    "            # Save the LoRA adapter only to save space\n",
    "            model_pegasus.save_pretrained(os.path.join(self.config.root_dir, \"pegasus-samsum-model-lora\"))\n",
    "            tokenizer.save_pretrained(os.path.join(self.config.root_dir, \"tokenizer\"))\n",
    "            print(\"Training complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during training: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Try to save checkpoint even if training fails\n",
    "            try:\n",
    "                print(\"Attempting to save checkpoint after error...\")\n",
    "                model_pegasus.save_pretrained(os.path.join(self.config.root_dir, \"checkpoint-error\"))\n",
    "                tokenizer.save_pretrained(os.path.join(self.config.root_dir, \"tokenizer-error\"))\n",
    "            except:\n",
    "                print(\"Could not save checkpoint after error.\")\n",
    "    \n",
    "    def print_trainable_parameters(self, model):\n",
    "        \"\"\"\n",
    "        Prints the number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        trainable_params = 0\n",
    "        all_params = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params:,} || all params: {all_params:,} || trainable%: {100 * trainable_params / all_params:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce GTX 1650\n",
      "Total GPU memory: 4.00 GB\n",
      "Currently allocated: 0.00 GB\n",
      "Max allocated: 0.83 GB\n",
      "[2025-04-30 09:06:21,767: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-04-30 09:06:21,770: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-04-30 09:06:21,771: INFO: common: created directory at: artifacts]\n",
      "[2025-04-30 09:06:21,771: INFO: common: created directory at: artifacts/model_trainer]\n",
      "Initial GPU memory: 0.00 MB allocated\n",
      "Using device: cuda\n",
      "Loading tokenizer...\n",
      "Configuring 4-bit quantization...\n",
      "Loading model with 4-bit quantization...\n",
      "[2025-04-30 09:06:24,111: INFO: modeling: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for training with PEFT/LoRA...\n",
      "Configuring LoRA with reduced parameters...\n",
      "Applying LoRA adapter to the model...\n",
      "Enabling gradient checkpointing...\n",
      "trainable params: 786,432 || all params: 336,702,464 || trainable%: 0.23%\n",
      "GPU memory after loading model: 620.61 MB allocated\n",
      "Creating data collator...\n",
      "Loading dataset...\n",
      "Using 1000 examples for training (out of 14732)\n",
      "Using 50 examples for validation (out of 818)\n",
      "Setting up trainer with batch size: 1, grad_accum: 32\n",
      "Creating trainer with ROUGE metrics...\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 22:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and tokenizer...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Make sure CUDA memory is clean before starting\n",
    "    import torch\n",
    "    import gc\n",
    "    import os\n",
    "    \n",
    "    # First make sure the rouge-score is installed\n",
    "    try:\n",
    "        import rouge_score\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        import sys\n",
    "        print(\"Installing rouge-score...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rouge-score\"])\n",
    "    \n",
    "    # Force aggressive garbage collection and empty CUDA cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Set memory efficient options\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:32\"\n",
    "    \n",
    "    # Check available GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Currently allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"Max allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Fix potential deadlocks in dataloader\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    \n",
    "    # Get configuration and create trainer\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    \n",
    "    # Train with all optimizations enabled\n",
    "    model_trainer.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    # Print the full traceback for debugging\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autosumai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
